\documentclass{article}
% !TEX root = homework_1.tex
\newif\ifshowsolutions
\showsolutionstrue
\input{../common/preamble}

\chead{
  {\vbox{
      \vspace{2mm}
      \large
      Machine Learning in Physics \hfill
      UCSD PHYS 139/239 \hfill \\[1pt]
      Homework 1\hfill
      Due: Friday, October 10, 2025, 8:00pm\\
	  \hfill
	  Corrections due: Wednesday, October 15, 2025, 8:00pm\\
    }
  }
}

\begin{document}
\pagestyle{fancy}

\section*{Policies}
\begin{itemize}
	\item You are free to collaborate on all of the problems, subject to the collaboration policy stated in the syllabus.
	\item Please submit your report as a single .pdf file to Gradescope under ``Homework 1" or ``Homework 1 Corrections".
	      \textbf{In the report, include any images generated by your code along with your answers to the questions.}
	      For instructions specifically pertaining to the Gradescope submission process, see \url{https://www.gradescope.com/get_started#student-submission}.
	\item Please submit your code as a .zip archive to Gradescope under ``Homework 1 Code'' or ``Homework 1 Code Corrections".
	      The .zip file should contain your code files.
	      Submit your code either as Jupyter notebook .ipynb files or .py files.
\end{itemize}

\newpage
\section{Basics [16 Points]}
\materials{lecture 1}

Answer each of the following problems with 1--2 short sentences.

\begin{problem}[2]
What is a hypothesis set?`'
\end{problem}
\begin{solution}
	The hypothesis set is the space of all possible functions that the model can fit to with its given architecture.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[2]
What is the hypothesis set of a linear model?
\end{problem}
\begin{solution}
	The hypothesis set of a linear model is the space of all possible linear functions.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[2]
What is overfitting?
\end{problem}
\begin{solution}
	Overfitting is when a model fits to a given dataset too closely, such that now it is tuned for the specific dataset and not
	the general behavior of the whole dataset and unseen data. 
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[2]
What are two ways to prevent overfitting?
\end{problem}
\begin{solution}
	You can prevent overfitting by simplifying the model so that it has less parameters to fit to the data, or use more data so that
	its more difficult to fit accurately to the entire dataset. Using fewer parameters simplifies the hypothesis set so that the model
	has less freedom to fit to the training data.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[2]
	What are training data and test data, and how are they used differently?
	Why should you never change your model based on information from test data?
\end{problem}
\begin{solution}
	Training data is the data that one uses to train the model and update its parameters on, while test data is the data that one uses 
	to evaluate the model's performance on. You should never change the model based on results of the test data because then it becomes
	training data, and the model begins to fit to the test dataset which will always be incorrectly more accurate than to real unseen data.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[2]
What are the two assumptions we make about how our dataset is sampled?
\end{problem}
\begin{solution}
	\begin{enumerate}
		\item The data is sampled independently and identically distributed.
		\item The data is representative of the unseen data it would be evaluating. 
	\end{enumerate}
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[2]
Consider the machine learning problem of classifying neutrino interaction events as in slide 14 of lecture 1.
What could $X$, the input space, be? What could $Y$, the output space, be?
\end{problem}
\begin{solution}
	The input space $X$ could be features of the raw data gathered from the detector and the output space $Y$ could be a class 
	of the interaction type. 
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[2]
What is the $k$-fold cross-validation procedure?
\end{problem}
\begin{solution}
	$k$-fold cross-validation is when one splits a dataset into $k$ equally sized subsets and trains on $k-1$ of the subsets and evaluates 
	on the remaining subset. This is repeated $k$ times so that each subset is used as the test set once.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}





\newpage
\section{Bias-Variance Tradeoff [39 Points]}
\materials{lecture 1}

\begin{problem}[5]
Derive the bias-variance decomposition for the squared error loss function.
That is, show that for a model $f_S$ trained on a dataset $S$ to predict a target $y(x)$ for each $x$ in the input space $\mathcal{X}$,
\begin{align}
	\E_S \left[E_\text{out}\left(f_S\right)\right] & = \E_x[\Bias(x) + \Var(x)]
\end{align}
given that $F(x)$ is the ``average function'' over all possible datasets $S$
\begin{align}
	F(x) & = \E_S\left[f_S(x) \right]
\end{align}
the out-of-sample error for a particular trained model is
\begin{align}
	E_\text{out}(f_S) & = \E_x\left[\left(f_S(x) - y(x)\right)^2\right]
\end{align}
and we define the bias for a given data sample $\Bias(x)$ by how much the average function deviates from the target function
\begin{align}
	\Bias(x) & = (F(x) - y(x))^2
\end{align}
and the variance for a given data sample $\Var(x)$ measures
\begin{align}
	\Var(x) & = \E_S\left[(f_S(x) - F(x))^2\right]
\end{align}
\end{problem}

\begin{solution}
	\noindent We start from the out-of-sample squared error of a model trained on a random
	dataset $S$:
	\[
	E_{\text{out}}(f_S)=\E_x\!\left[(f_S(x)-y(x))^2\right].
	\]
	We want its expectation over datasets $S$:
	\[
	\E_S\!\left[E_{\text{out}}(f_S)\right]
	=\E_S\E_x\!\left[(f_S(x)-y(x))^2\right]
	\]

	\noindent Expand the squared term by adding and subtracting the average predictor F(x)=ES[fS(x)]
	\[
	(f_S(x)-y(x))^2 = (f_S(x)-F(x)+F(x)-y(x))^2
	\]
	\[
	(f_S(x)-F(x)+F(x)-y(x))^2 = (f_S(x)-F(x))^2 + 2(f_S(x)-F(x))(F(x)-y(x)) + (F(x)-y(x))^2
	\]

	\noindent By definition of $F(x)$, $\E_S[f_S(x)-F(x)]=0$, so the cross term vanishes.
	\[2(f_S(x)-F(x))(F(x)-y(x)) = 0\]

	\noindent So now we are left with 
	\[(f_S(x)-y(x))^2 = (f_S(x)-F(x))^2 + (F(x)-y(x))^2\]

	\noindent Taking expectation over $x$ gives
	\[
	\E_S\!\left[(f_S(x)-F(x))^2\right] = \E_S\!\left[(f_S(x)-F(x))^2\right] + \E_S\!\left[(F(x)-y(x))^2\right]
	\]	

	\noindent and we know that 
	\[
	\E_S\!\left[(f_S(x)-F(x))^2\right] = \Var(x)
	\]
	\[
	\E_S\!\left[(F(x)-y(x))^2\right] = \Bias(x)
	\]
	\noindent so we have:
	\[
	\E_S\!\left[E_{\text{out}}(f_S)\right]
	=\E_x\!\left[\Bias(x)+\Var(x)\right]
	\]
	which is the desired bias–variance decomposition.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}

\end{solution}
	



\begin{problem}[5] When there is noise in the data, the out-of-sample error is
\begin{align}
	E_\text{out}(f_S)  = \E_{x,z}[(f_S(x) - z(x))^2]
\end{align}
where $z(x) = y(x) + \epsilon$.
If $\epsilon$ is a Gaussian-distributed random variable with mean of zero and variance $\sigma^2$, show that the bias-variance decomposition becomes
\begin{align}
	\E_S \left[E_\text{out}\left(f_S\right)\right] & = \E_x[\Bias(x) + \Var(x)] + \sigma^2
\end{align}
\begin{hint}
	Given the mean of $\epsilon$ is zero,
	\begin{align}
		\E_{x, z}[\epsilon] & = \E_{x, \epsilon}[\epsilon] = \E_\epsilon [\epsilon] = 0
	\end{align}
	Likewise,
	\begin{align}
		\E_{x, z}[\epsilon^2] & = \E_\epsilon [\epsilon^2] = \E_\epsilon [(\epsilon- \E_\epsilon[\epsilon])^2] = \sigma^2
	\end{align} by the definition of variance.
\end{hint}
\end{problem}
\begin{solution}
	\noindent When there is noise in the data, we assume that the observed target is 
	\[
	z(x) = y(x) + \epsilon,
	\]
	where $\epsilon$ is a Gaussian random variable with mean $0$ and variance $\sigma^2$. 
	
	\noindent The out-of-sample error of a model trained on a random dataset $S$ is then
	\[
	E_{\text{out}}(f_S) = \E_{x,z}\!\left[(f_S(x) - z(x))^2\right].
	\]
	
	\noindent Substituting $z(x) = y(x) + \epsilon$ gives
	\[
	E_{\text{out}}(f_S) = \E_{x,\epsilon}\!\left[(f_S(x) - y(x) - \epsilon)^2\right].
	\]
	
	\noindent Taking the expectation over datasets $S$, we have
	\[
	\E_S\!\left[E_{\text{out}}(f_S)\right]
	= \E_S\E_{x,\epsilon}\!\left[(f_S(x) - y(x) - \epsilon)^2\right].
	\]
	
	\noindent Expanding the squared term:
	\[
	(f_S(x) - y(x) - \epsilon)^2 
	= (f_S(x) - y(x))^2 - 2\epsilon(f_S(x)-y(x)) + \epsilon^2.
	\]
	
	\noindent Taking the expectation with respect to $\epsilon$, and using $\E_\epsilon[\epsilon] = 0$ and $\E_\epsilon[\epsilon^2] = \sigma^2$, we get
	\[
	\E_\epsilon[(f_S(x) - y(x) - \epsilon)^2]
	= (f_S(x) - y(x))^2 + \sigma^2.
	\]
	
	\noindent Therefore,
	\[
	\E_S\!\left[E_{\text{out}}(f_S)\right]
	= \E_S\E_x\!\left[(f_S(x) - y(x))^2\right] + \sigma^2.
	\]
	
	\noindent The first term is the same as in the noiseless case. Expanding it by adding and subtracting the average predictor $F(x) = \E_S[f_S(x)]$, we have
	\[
	(f_S(x)-y(x))^2 = (f_S(x)-F(x))^2 + 2(f_S(x)-F(x))(F(x)-y(x)) + (F(x)-y(x))^2.
	\]
	
	\noindent Since $\E_S[f_S(x)-F(x)] = 0$, the cross term vanishes, and we are left with
	\[
	\E_S[(f_S(x)-y(x))^2] = \E_S[(f_S(x)-F(x))^2] + (F(x)-y(x))^2.
	\]
	
	\noindent Recognizing these as the variance and bias terms, respectively,
	\[
	\E_S[(f_S(x)-F(x))^2] = \Var(x), \quad (F(x)-y(x))^2 = \Bias(x),
	\]
	we obtain
	\[
	\E_S\!\left[E_{\text{out}}(f_S)\right] = \E_x[\Bias(x) + \Var(x)] + \sigma^2.
	\]
	
	\noindent Thus, when noise is present, the bias–variance decomposition becomes
	\[
	\boxed{\E_S[E_{\text{out}}(f_S)] = \E_x[\Bias(x) + \Var(x)] + \sigma^2,}
	\]
	where $\sigma^2$ is the \textit{irreducible error} introduced by the data noise.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
	
\end{solution}


\begin{problem}[14]
In the following problems, you will explore the bias-variance tradeoff by producing learning curves for polynomial regression models.

A \emph{learning curve} for a model is a plot showing both the training error and the cross-validation error as a function of the number of points in the training set.
These plots provide valuable information regarding the bias and variance of a model and can help determine whether a model is over- or under-fitting.

\emph{Polynomial regression} is a type of regression that models the target $y$ as a degree-$d$ polynomial function of the input $x$.
(The modeler chooses $d$.)
You don't need to know how it works for this problem, just know that it produces a polynomial that attempts to fit the data.

Use the provided \texttt{2\_notebook.ipynb} Jupyter notebook to enter your code for this question.
This notebook contains examples of using NumPy's \texttt{polyfit} and \texttt{polyval} methods, and Scikit-learn's \texttt{KFold} method; you may find it helpful to read through and run this example code prior to continuing with this problem.
Additionally, you may find it helpful to look at the documentation for Scikit-learn's \texttt{learning\_curve} method for some guidance.

The dataset \texttt{bv\_data.csv} is provided and has a header denoting which columns correspond to which values. Using this dataset, plot learning curves for 1st-, 2nd-, 6th-, and 12th-degree polynomial regression (4 separate plots) by following these steps for each degree $d \in \{1, 2, 6, 12\}$:

\begin{enumerate}
	\item For each $N \in \{20, 25, 30, 35, \ldots, 100\}$:
	      \begin{enumerate}[i.]
		      \item Perform 5-fold cross-validation on the first $N$ points in the dataset (setting aside the other points), computing the both the training and validation error for each fold.
		            \begin{itemize}
			            \item Use the mean squared error loss as the error function.
			            \item Use NumPy's \texttt{polyfit} method to perform the degree-$d$ polynomial regression and NumPy's \texttt{polyval} method to help compute the errors.
			                  (See the example code and \href{https://docs.scipy.org/doc/NumPy/reference/routines.polynomials.poly1d.html}{NumPy documentation} for details.)
			            \item When partitioning your data into folds, although in practice you should randomize your partitions, for the purposes of this set, simply divide the data into $K$ contiguous blocks.
		            \end{itemize}
		      \item Compute the average of the training and validation errors from the 5 folds.
	      \end{enumerate}
	\item Create a learning curve by plotting both the average training and validation error as functions of $N$.
\end{enumerate}

\end{problem}
\begin{solution}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{code/learning_curve_degree_1.png}
		\caption{Learning curve for degree-2 polynomial regression}
		\label{fig:learning_curve}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{code/learning_curve_degree_2.png}
		\caption{Learning curve for degree-2 polynomial regression}
		\label{fig:learning_curve_degree_2}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{code/learning_curve_degree_6.png}
		\caption{Learning curve for degree-6 polynomial regression}
		\label{fig:learning_curve_degree_6}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{code/learning_curve_degree_12.png}
		\caption{Learning curve for degree-12 polynomial regression}
		\label{fig:learning_curve_degree_12}
	\end{figure}

	\vspace{0.3cm}
	\noindent\textcolor{red}{\textbf{Corrections:}}
	
	\noindent The following corrections were made to avoid numerical instability issues:
	
	\begin{enumerate}
		\item \textbf{Starting N at 20:} The range of training points now starts at N=20 instead of N=5. This ensures that even with 5-fold cross-validation, each training fold has at least 16 data points.
		
		\item \textbf{Y-axis limits:} Added \texttt{plt.ylim(0, 6)} to clip the y-axis range, making the plots readable even when high-degree polynomials (especially degree 12) produce very large validation errors at small N values.
		
		\item \textbf{Reason for changes:} With very small N values (like N=5 or N=10), 5-fold cross-validation leaves too few points in each training fold for high-degree polynomials. For example:
		\begin{itemize}
			\item At N=15 with 5-fold CV: 12 training points per fold
			\item Degree 12 polynomial needs 13 parameters
			\item This creates an underdetermined system leading to numerical instability and massive errors (in the millions/billions)
		\end{itemize}
	\end{enumerate}
	
	\noindent By starting at N=20, we ensure sufficient training data for all polynomial degrees tested.
	
	\vspace{0.3cm}
	
\end{solution}



\begin{problem}[3]
Based on the learning curves, which polynomial regression model (i.e. which degree polynomial) has the highest bias? How can you tell?
\end{problem}
\begin{solution}
	The linear model has the highest bias. We can tell because the training error is the highest and the validation error is the lowest. 
	them is small.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[3]
Which model has the highest variance? How can you tell?
\end{problem}
\begin{solution}
	The 12th degree model has the highest variance. We can tell because the training error is very low but the validation error is very high.
	it is overfitting to the training data and not generalizing to the validation data.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
	
\end{solution}

\begin{problem}[3]
What does the learning curve of the quadratic model tell you about how much the model will improve if we had additional training points?
\end{problem}
\begin{solution}
	The quadratic model will improve by a little bit as we add more training points but there are diminishing returns.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[3]
Why is training error generally lower than validation error?
\end{problem}
\begin{solution}
	Because the model is training on the training data and actively adjusting to fit it.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[3]
Based on the learning curves, which model would you expect to perform best on some unseen data drawn from the same distribution as the training data, and why?
\end{problem}
\begin{solution}
	The quadratic model would perform best on some unseen data drawn from the same distribution as the training data.
	Because it has the lowest bias and variance meaning the lowest average error. 
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}













\newpage
\section{The Perceptron [14 Points]}
\materials{lecture 2}

The perceptron is a simple linear model used for binary classification.
For an input vector $\mathbf{x} \in \mathbb{R}^d$, weights $\mathbf{w} \in \mathbb{R}^d$, and bias $b \in \mathbb{R}$, a perceptron $f: \mathbb{R}^d \rightarrow \{-1,1\}$ takes the form
\begin{align*}
	f(\mathbf{x}) = \operatorname{sign}\left(\left(\sum_{i=1}^d w_i x_i\right) + b \right)
\end{align*}

The weights and bias of a perceptron can be thought of as defining a hyperplane that divides $\mathbb{R}^d$ such that each side represents an output class. For example, for a two dimensional dataset, a perceptron could be drawn as a line that separates all points of class $+1$ from all points of class $-1$.

The perceptron learning algorithm (PLA) is a simple method of training a perceptron.
First, an initial guess is made for the weight vector $\mathbf{w}$.
Then, one misclassified point is chosen arbitrarily and the $\mathbf{w}$ vector is updated by
\begin{align*}
	\mathbf{w}_{t+1} & = \mathbf{w}_t + y(t)\mathbf{x}(t) \\
	b_{t + 1}        & = b_t + y(t),
\end{align*}

where $\mathbf{x}(t)$ and $y(t)$ correspond to the misclassified point selected at the $t^\text{th}$ iteration.
This process continues until all points are classified correctly.

The following few problems ask you to work with the provided Jupyter notebook for this problem, titled \texttt{3\_notebook.ipynb}. This notebook utilizes the file \texttt{perceptron\_helper.py}, but you should not need to modify this file.

\begin{problem}[8]
The graph below shows an example 2D dataset.
The $+$ points are in the $+1$ class and the $\circ$ point is in the $-1$ class.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{images/perceptron.png}
	\caption{The green $+$ are positive and the red $\circ$ is negative}
	\label{fig:figure1}
\end{figure}

Implement the \texttt{update\_perceptron} and \texttt{run\_perceptron} methods in the notebook, and perform the perceptron algorithm with initial weights $w_1 = 0, w_2 = 1, b = 0$.

Give your solution in the form a table showing the weights and bias at each time step and the misclassified point $([x_1,x_2],y)$ that is chosen for the next iteration's update.
You can iterate through the three points in any order.
Your code should output the values in the table below; cross-check your answer with the table to confirm that your perceptron code is operating correctly.

\begin{table}[H]
	\centering

	\begin{tabular}{l|lll|ll|l}
		\hline

		\hline
		$t$ & $b$ & $w_1$ & $w_2$ & $x_1$ & $x_2$ & $y$  \\
		\hline
		0   & 0   & 0     & 1     & 1     & $-2$  & $+1$ \\
		1   & 1   & 1     & $-1$  & 0     & 3     & $+1$ \\
		2   & 2   & 1     & 2     & 1     & $-2$  & $+1$ \\
		3   & 3   & 2     & 0                            \\
		\hline
	\end{tabular}
\end{table}

Include in your report both: the table that your code outputs, as well as the plots showing the perceptron's classifier at each step (see notebook for more detail).


\end{problem}
\begin{solution}

	\noindent Python Output:
	
	\noindent bias: 1.0, weights: [ 1. -1.], point: [ 1 -2], label: 1 \\
	\noindent bias: 2.0, weights: [1. 2.], point: [0 3], label: 1 \\
	\noindent bias: 3.0, weights: [2. 0.], point: [ 1 -2], label: 1 \\

	\noindent final w = [2. 0.], final b = 3.0 \\


	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{code/perceptron_iter.png}
		\caption{Perceptron plot}
		\label{fig:perceptron_plot}
	\end{figure}
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}

\end{solution}

\begin{problem}[4]
A dataset $S = \{(\mathbf{x}_1, y_1),\cdots,(\mathbf{x}_N, y_N)\} \subset \mathbb{R}^d \times \mathbb{R}$ is \emph{linearly separable} if there exists a perceptron that correctly classifies all data points in the set.
In other words, there exists a hyperplane that separates positive data points and negative data points.

In a 2D dataset, how many data points are in the smallest dataset that is not linearly separable, such that no three points are collinear? How about for a 3D dataset such that no four points are coplanar?
Please limit your solution to a few lines---you should justify but not prove your answer.

Finally, how does this generalize for an $N$-dimensional set, in which \textbf{no} $<N$-dimensional hyperplane contains a non-linearly-separable subset?
For the $N$-dimensional case, you may state your answer without proof or justification.
\end{problem}
\begin{solution}
	for a 2D dataset, the smallest dataset that is not linearly separable is 4 points.
	for a 3D dataset, the smallest dataset that is not linearly separable is 5 points.
	for an $N$-dimensional dataset, the smallest dataset that is not linearly separable is $N+2$ points.

	For a 2D dataset, if you arrange 3 points such that they are not collinear, then you can always separate two classes with a line because they form a triangle.
	Therefore the smallest dataset that is not linearly separable is 4 points. This generalizes to higher dimensions.
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}
\end{solution}

\begin{problem}[2]
Run the visualization code in the Jupyter notebook section corresponding to question C (report your plots).
Assume a dataset is \emph{not} linearly separable.
Will the PLA ever converge?
Why or why not?
\end{problem}
\begin{solution}

	The PLA will never converge because the dataset is not linearly separable.

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{code/not_separable.png}
		\caption{Perceptron plot}
		\label{fig:perceptron_non_separable}
	\end{figure}
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}

\end{solution}















\newpage
\section{TensorFlow Playground [14 Points]}

The purpose of this problem is to build some intuition around linear models and neural networks.

\begin{problem}[6]
Navigate your web browser to the TensorFlow Playground (\url{https://playground.tensorflow.org/#networkShape=&dataset=xor&discretize=true}).
Select the checkerboard pattern for the data, which is known as the XOR dataset.
The data is sampled from a 2D probability density distribution represented by $(x_1, x_2)$.
The regions $x_1, x_2 > 0$ and $x_1, x_2 < 0$ have a target value $y = +1$ and are shown as blue data points, while the regions $x_1 > 0, x_2 < 0$ and $x_1 < 0, x_2 > 0$ have a target value of $y=-1$ and are shown as orange data points.

First, select a linear model with no hidden layers.
For the features, select the two independent variables $x_1$ and $x_2$.
Can you fit the data with this linear model?
Why or why not?
What happens if you add the feature $x_1x_2$?

Now, return the features to just $(x_1, x_2)$ and start adding hidden layers.
What's the smallest neural network (least number of layers and least number of neurons per layer) you can create that fits the training data ``perfectly'' (i.e. a training loss ${<}0.001$)? What is the corresponding test loss?
Detail your hyperparameter choices by providing a screenshot and the URL to your solution (the URL contains all your settings choices).
\end{problem}

\begin{solution}
	You cannot separate the data with a linear model because it can only separate data with a hyperplane. This is analogous to the previous question where
	you have a different class to the adjacent one on each corner of the square.

	Once you add $x_1*x_2$ as a feature, you can separate the data with a linear model. Because $x_1*x_2$ is a non-linear feature.

	The smallest neural network that can fit the data is a 1 hidden layer network with 5 neurons. 
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{code/checkerboard.png}
		\caption{Linear model}
		\label{fig:linear_model}
	\end{figure}

	Link:
	\url{https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=5&seed=0.09614&showTestData=false&discretize=true&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false}
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}

\end{solution}

\begin{problem}[8]
Navigate your web browser to the TensorFlow Playground \url{https://playground.tensorflow.org/#dataset=spiral&discretize=true}.
Select the spiral pattern for the data.

Using all the features available, find a solution that fits the training data.
What training and test error do you achieve?
Is this a low bias/high variance or high bias/low variance model?
How do you know?

Using only $(x_1, x_2)$, find a solution that fits the training data.
What training and test error do you achieve?
Is this a low bias/high variance or high bias/low variance model?
How do you know?

For both solutions, detail your hyperparameter choices by providing a screenshot and the URL to your solution (the URL contains all your settings choices).

\textbf{Bonus}:
Can you find two engineered features that would allow you to find a solution with a linear model?
\end{problem}

\begin{solution}

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{code/spiral_1.png}
		\caption{Spiral model}
		\label{fig:spiral_model}
	\end{figure}

	Link:
	\url{https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=5&seed=0.69944&showTestData=false&discretize=true&percTrainData=90&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false}
	



	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{code/spiral_2.png}
		\caption{Spiral model}
		\label{fig:spiral_model}
	\end{figure}
	
	Link:
	\url{https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8,8&seed=0.69944&showTestData=false&discretize=true&percTrainData=90&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false}

	This first model has higher bias and lower variance, and the second model has lower bias and higher variance. This is because the first model
	has fewer layers and neurons. 
	
	\vspace{0.2cm}
	\noindent\textcolor{red}{\textbf{Correct or equivalent solution}}

\end{solution}

\end{document}
